{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "267ef930",
      "metadata": {
        "id": "267ef930"
      },
      "source": [
        "# Encoder–Decoder RNN Demo (Toy Neural Machine Translation)\n",
        "\n",
        "This notebook demonstrates the **idea** of an encoder–decoder RNN for neural machine translation,\n",
        "similar to the diagram:\n",
        "\n",
        "`I  → love → llamas  → [Encoder RNN] → [Decoder RNN] →  Ik → hou → van → lama's`\n",
        "\n",
        "The model here is **not trained**. Instead, we:\n",
        "- Use simple RNN equations with random weights, just to show the *flow* of information.\n",
        "- **Force** the decoder to output the Dutch sequence `['Ik', 'hou', 'van', \"lama's\"]`\n",
        "  so that it exactly matches the teaching diagram.\n",
        "\n",
        "This is meant purely as a teaching tool, not a real translation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a2f6a731",
      "metadata": {
        "id": "a2f6a731"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ---- English → Dutch toy vocabularies ----\n",
        "en_vocab = {\"I\": 0, \"love\": 1, \"llamas\": 2}\n",
        "nl_vocab = {\"<BOS>\": 0, \"Ik\": 1, \"hou\": 2, \"van\": 3, \"lama's\": 4}\n",
        "\n",
        "id2en = {i: w for w, i in en_vocab.items()}\n",
        "id2nl = {i: w for w, i in nl_vocab.items()}\n",
        "\n",
        "def one_hot(index, size):\n",
        "    v = np.zeros(size)\n",
        "    v[index] = 1.0\n",
        "    return v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ed0c8c0",
      "metadata": {
        "id": "4ed0c8c0",
        "outputId": "147a270a-b5a8-41ce-c7cb-26d88b7e6ffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ENCODER ===\n",
            "Input word: I        →  hidden state: [0.707 0.197 0.454 0.808]\n",
            "Input word: love     →  hidden state: [ 0.934 -0.719  0.705 -0.278]\n",
            "Input word: llamas   →  hidden state: [-0.896  0.646  0.434  0.742]\n",
            "\n",
            "Final encoder hidden state (context vector):\n",
            "[-0.896  0.646  0.434  0.742]\n"
          ]
        }
      ],
      "source": [
        "# ================= ENCODER (RNN) =================\n",
        "# Task: 'representing language' – turn the input sentence into a context vector.\n",
        "\n",
        "input_size  = len(en_vocab)   # one-hot size for English\n",
        "hidden_size = 4               # small hidden state for illustration\n",
        "\n",
        "np.random.seed(0)  # keep results deterministic for class\n",
        "\n",
        "# Random weights for the encoder RNN\n",
        "W_xh = np.random.randn(input_size, hidden_size) * 0.5\n",
        "W_hh = np.random.randn(hidden_size, hidden_size) * 0.5\n",
        "b_h  = np.zeros(hidden_size)\n",
        "\n",
        "def rnn_step(x_t, h_prev):\n",
        "    \"\"\"Single RNN step: h_t = tanh(W_xh x_t + W_hh h_{t-1} + b).\"\"\"\n",
        "    return np.tanh(x_t @ W_xh + h_prev @ W_hh + b_h)\n",
        "\n",
        "encoder_inputs = [\"I\", \"love\", \"llamas\"]\n",
        "h = np.zeros(hidden_size)   # initial hidden state (all zeros)\n",
        "\n",
        "print(\"=== ENCODER ===\")\n",
        "for token in encoder_inputs:\n",
        "    x_t = one_hot(en_vocab[token], input_size)\n",
        "    h = rnn_step(x_t, h)\n",
        "    print(f\"Input word: {token:7s}  →  hidden state: {np.round(h, 3)}\")\n",
        "\n",
        "encoder_final_state = h\n",
        "print(\"\\nFinal encoder hidden state (context vector):\")\n",
        "print(np.round(encoder_final_state, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "291719d1",
      "metadata": {
        "id": "291719d1",
        "outputId": "caf1042b-a86b-41bf-ac30-e2c52e58772d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DECODER (forced outputs) ===\n",
            "Step 1: input token = <BOS>  → output (forced) = Ik     , hidden = [ 0.797  0.363  0.03  -0.511]\n",
            "Step 2: input token = Ik     → output (forced) = hou    , hidden = [-0.764 -0.832 -0.325  0.633]\n",
            "Step 3: input token = hou    → output (forced) = van    , hidden = [ 0.84   0.766  0.006 -0.746]\n",
            "Step 4: input token = van    → output (forced) = lama's , hidden = [-0.817 -0.806 -0.735  0.96 ]\n",
            "\n",
            "Final decoded sequence:\n",
            "['Ik', 'hou', 'van', \"lama's\"]\n"
          ]
        }
      ],
      "source": [
        "# ================= DECODER (RNN) =================\n",
        "# Task: 'generating language' – turn the context vector into the output sentence.\n",
        "#\n",
        "# NOTE: In a real system, the decoder would *learn* to output the Dutch words.\n",
        "# Here we **force** the decoder to output ['Ik', 'hou', 'van', \"lama's\"]\n",
        "# so it exactly matches the teaching diagram.\n",
        "\n",
        "decoder_input_size = len(nl_vocab)\n",
        "\n",
        "# Separate (random) weights for the decoder RNN\n",
        "W_xh_dec = np.random.randn(decoder_input_size, hidden_size) * 0.5\n",
        "W_hh_dec = np.random.randn(hidden_size, hidden_size) * 0.5\n",
        "b_h_dec  = np.zeros(hidden_size)\n",
        "\n",
        "def rnn_step_dec(x_t, h_prev):\n",
        "    return np.tanh(x_t @ W_xh_dec + h_prev @ W_hh_dec + b_h_dec)\n",
        "\n",
        "print(\"\\n=== DECODER (forced outputs) ===\")\n",
        "target_sequence = [\"Ik\", \"hou\", \"van\", \"lama's\"]\n",
        "decoded_words = []\n",
        "\n",
        "h_dec = encoder_final_state.copy()  # start from encoder context\n",
        "prev_token = \"<BOS>\"                # begin-of-sentence token\n",
        "\n",
        "for step, target_word in enumerate(target_sequence, start=1):\n",
        "    # Standard decoder RNN update\n",
        "    x_t = one_hot(nl_vocab[prev_token], decoder_input_size)\n",
        "    h_dec = rnn_step_dec(x_t, h_dec)\n",
        "    # Instead of choosing from a softmax, we FORCE the target word\n",
        "    next_word = target_word\n",
        "\n",
        "    print(\n",
        "        f\"Step {step}: input token = {prev_token:6s} → output (forced) = {next_word:7s}, \"\n",
        "        f\"hidden = {np.round(h_dec, 3)}\"\n",
        "    )\n",
        "\n",
        "    decoded_words.append(next_word)\n",
        "    prev_token = next_word\n",
        "\n",
        "print(\"\\nFinal decoded sequence:\")\n",
        "print(decoded_words)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}